<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="referrer" content="never" />
    <meta property="og:description" content="本文整理了入门python机器学习最基本的算法，可作为手册使用 梳理了python代码，方便快速从这个手册中构建出相应的代码应用于自己的项目中 自己梳理记录了很长时间，真的很精华，分享给大家，欢迎评论" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>机器学习经典算法大全-代码整理（python） - 唐啊唐囧囧 - 博客园</title>
    
    <link rel="stylesheet" href="/css/blog-common.min.css?v=-oFz8B4m7JhHaZzdTkzPza2oLZNDRR8obnCz6w7OHbU" />
    <link id="MainCss" rel="stylesheet" href="/skins/lessismoreright/bundle-lessismoreright.min.css?v=r7HROo-7bHpj9wQ7aMjjOlbCdKigSxO_AFRYzTkcpKo" />
    
    <link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="/skins/lessismoreright/bundle-lessismoreright-mobile.min.css?v=FJjyQba01xzuYKRyPpSwE1bMq69pRjxrz5wp2oZZGLY" />
    
    <link type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/tangg/rss" />
    <link type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/tangg/rsd.xml" />
    <link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/tangg/wlwmanifest.xml" />
    <script src="https://common.cnblogs.com/scripts/jquery-2.2.0.min.js"></script>
    <script src="/js/blog-common.min.js?v=z6JkvKQ7L_bGD-nwJExYzsoFf5qnluqZJru6RsfoZuM"></script>
    <script>
        var currentBlogId = 566870;
        var currentBlogApp = 'tangg';
        var cb_enable_mathjax = true;
        var isLogined = false;
        var skinName = 'LessIsMoreRight';
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processClass: 'math', processEscapes: true },
        TeX: {
        equationNumbers: { autoNumber: ['AMS'], useLabelIds: true },
        extensions: ['extpfeil.js', 'mediawiki-texvc.js'],
        Macros: {bm: "\\boldsymbol"}
        },
        'HTML-CSS': { linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic: true } }
        });
    </script>
    <script src="https://mathjax.cnblogs.com/2_7_5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
</head>
<body>
    <a name="top"></a>
    
    <div id="home">
    <div id="header">
        <div id="blogTitle">
            
<div class="title"><a id="Header1_HeaderTitle" class="headermaintitle HeaderMainTitle" href="https://www.cnblogs.com/tangg/">唐啊唐囧囧</a>
</div>
<div class="subtitle">

</div>

        </div>
        <div id="navigator">
            
<ul id="navList">
    <li id="nav_sitehome"><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">
博客园</a>
</li>
    <li id="nav_myhome">
<a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/tangg/">
首页</a>
</li>
    <li id="nav_newpost">

<a id="blog_nav_newpost" class="menu" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">
新随笔</a>
</li>
    <li id="nav_contact">
<a id="blog_nav_contact" class="menu" href="https://msg.cnblogs.com/send/%E5%94%90%E5%95%8A%E5%94%90%E5%9B%A7%E5%9B%A7">
联系</a></li>
    <li id="nav_rss">
<a id="blog_nav_rss" class="menu" href="https://www.cnblogs.com/tangg/rss/">
订阅</a></li>
    <li id="nav_admin">
<a id="blog_nav_admin" class="menu" href="https://i.cnblogs.com/">
管理</a>
</li>
</ul>

            <div class="blogStats">
                
<span id="stats_post_count">随笔 - 
6&nbsp;</span>
<span id="stats_article_count">文章 - 
0&nbsp;</span>
<!-- <span id="stats-comment_count"></span> -->
<span id="stats_comment_count">评论 - 
0</span>
            </div>
        </div>
    </div>
    <div id="main">
        <div id="mainContent">
            <div class="forFlow">
                <div id="post_detail">
    <div id="topics">
        <div class="post">
            <h1 class="postTitle">
                
<a id="cb_post_title_url" class="postTitle2" href="https://www.cnblogs.com/tangg/p/12782862.html">机器学习经典算法大全-代码整理（python）</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                
<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
    <p>本文整理了入门python机器学习最基本的算法，可作为手册使用</p>
<p>梳理了python代码，方便快速从这个手册中构建出相应的代码应用于自己的项目中</p>
<p>自己梳理记录了很长时间，真的很精华，分享给大家，欢迎评论留言</p>
<p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#1knn-分类算法">1.KNN 分类算法</a><ul><li><a href="#11-归一化处理-scaler">1.1 归一化处理 scaler</a></li><li><a href="#12-网格搜索-gridsearchcv">1.2 网格搜索 GridSearchCV</a></li><li><a href="#13-交叉验证">1.3 交叉验证</a></li></ul></li><li><a href="#2-线性回归">2. 线性回归</a><ul><li><a href="#21-简单线性回归">2.1 简单线性回归</a></li><li><a href="#22-多元线性回归">2.2 多元线性回归</a></li></ul></li><li><a href="#3-梯度下降法">3. 梯度下降法</a><ul><li><a href="#31-随机梯度下降线性回归">3.1 随机梯度下降线性回归</a></li><li><a href="#32-确定梯度下降计算的准确性">3.2 确定梯度下降计算的准确性</a></li></ul></li><li><a href="#4-pca算法">4. PCA算法</a><ul><li><a href="#41-代码流程">4.1 代码流程</a></li><li><a href="#42-降维的维数和精度的取舍">4.2 降维的维数和精度的取舍</a></li><li><a href="#43-pca数据降噪">4.3 PCA数据降噪</a></li></ul></li><li><a href="#5-多项式回归与模型泛化">5. 多项式回归与模型泛化</a><ul><li><a href="#51-多项式回归和pipeline">5.1 多项式回归和Pipeline</a></li><li><a href="#52-gridsearchcv-和-pipeline">5.2 GridSearchCV 和 Pipeline</a></li><li><a href="#53-模型泛化之岭回归（ridge）">5.3 模型泛化之岭回归（Ridge）</a></li><li><a href="#54-模型泛化之lasso回归">5.4 模型泛化之LASSO回归</a></li></ul></li><li><a href="#6-逻辑回归">6. 逻辑回归</a><ul><li><a href="#61-绘制决策边界">6.1 绘制决策边界</a></li><li><a href="#62-多项式逻辑回归">6.2 多项式逻辑回归</a></li><li><a href="#63-逻辑回归中的正则化项和惩罚系数c">6.3 逻辑回归中的正则化项和惩罚系数C</a></li><li><a href="#64-ovr-和-ovo">6.4 OVR 和 OVO</a></li></ul></li><li><a href="#7-支撑向量机svm">7. 支撑向量机SVM</a><ul><li><a href="#71-svm的正则化">7.1 SVM的正则化</a></li><li><a href="#72-线性svm">7.2 线性SVM</a></li><li><a href="#73-多项式特征svm">7.3 多项式特征SVM</a><ul><li><a href="#731-传统pipeline多项式svm">7.3.1 传统Pipeline多项式SVM</a></li><li><a href="#732-多项式核函数svm">7.3.2 多项式核函数SVM</a></li><li><a href="#733-高斯核svm（rbf）">7.3.3 高斯核SVM（RBF）</a></li></ul></li><li><a href="#74-使用svm解决回归问题">7.4 使用SVM解决回归问题</a></li></ul></li><li><a href="#8-决策树">8. 决策树</a><ul><li><a href="#81-分类">8.1 分类</a></li><li><a href="#82-回归">8.2 回归</a></li></ul></li><li><a href="#9-集成学习和随机森林">9. 集成学习和随机森林</a><ul><li><a href="#91-hard-voting-classifier">9.1 Hard Voting Classifier</a></li><li><a href="#92-soft-voting-classifier">9.2 Soft Voting Classifier</a></li><li><a href="#93-bagging（放回取样）">9.3 Bagging（放回取样）</a></li><li><a href="#94-随机森林和extra-tree">9.4 随机森林和Extra-Tree</a><ul><li><a href="#941-随机森林">9.4.1 随机森林</a></li><li><a href="#942-extra-tree">9.4.2 Extra-Tree</a></li></ul></li><li><a href="#95-ada-boosting">9.5 Ada Boosting</a></li><li><a href="#96-gradient-boosting">9.6 Gradient Boosting</a></li><li><a href="#总结">总结</a><ul><li><a href="#例子：">例子：</a></li></ul></li></ul></li><li><a href="#10-k-means聚类">10. K-means聚类</a><ul><li><a href="#101-传统k-means聚类">10.1 传统K-means聚类</a></li><li><a href="#102-非线性边界聚类">10.2 非线性边界聚类</a></li><li><a href="#103-预测结果与真实标签的匹配">10.3 预测结果与真实标签的匹配</a></li><li><a href="#104-聚类结果的混淆矩阵">10.4 聚类结果的混淆矩阵</a></li><li><a href="#105-t分布邻域嵌入预处理">10.5 t分布邻域嵌入预处理</a></li></ul></li><li><a href="#11-高斯混合模型（聚类、密度估计）">11. 高斯混合模型（聚类、密度估计）</a><ul><li><a href="#111-观察k-means算法的缺陷">11.1 观察K-means算法的缺陷</a></li><li><a href="#112-引出高斯混合模型">11.2 引出高斯混合模型</a></li><li><a href="#113-将gmm用作密度估计">11.3 将GMM用作密度估计</a></li><li><a href="#114-由分布函数得到生成模型">11.4 由分布函数得到生成模型</a></li><li><a href="#115-需要多少成分？">11.5 需要多少成分？</a></li></ul></li><li><a href="#评价指标">评价指标</a><ul><li><a href="#一、分类算法">一、分类算法</a><ul><li><a href="#1分类准确度">1.分类准确度</a></li><li><a href="#2-混淆矩阵、精准率、召回率">2. 混淆矩阵、精准率、召回率</a></li><li><a href="#3f1-score">3.F1-score</a></li><li><a href="#4精准率和召回率的平衡">4.精准率和召回率的平衡</a></li><li><a href="#5精准率-召回率曲线（pr曲线）">5.精准率-召回率曲线（PR曲线）</a></li><li><a href="#6roc曲线">6.ROC曲线</a></li></ul></li><li><a href="#二、回归算法">二、回归算法</a><ul><li><a href="#1均方误差-mse">1.均方误差 MSE</a></li><li><a href="#2平均绝对值误差-mae">2.平均绝对值误差 MAE</a></li><li><a href="#3均方根误差-rmse">3.均方根误差 RMSE</a></li><li><a href="#4r2评分">4.R2评分</a></li><li><a href="#5学习曲线">5.学习曲线</a></li></ul></li></ul></li></ul></div></p>
<h1 id="1knn-分类算法">1.KNN 分类算法</h1>
<p>由于knn算法涉及到<strong>距离</strong>的概念，KNN 算法需要先进行<strong>归一化处理</strong></p>
<h2 id="11-归一化处理-scaler">1.1 归一化处理 scaler</h2>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

standardScaler =StandardScaler()

standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)
</code></pre>
<p>归一化之后送入模型进行训练</p>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier(n_neighbors=8)
knn_classifier.fit(X_train_standard, y_train)
y_predict = knn_clf.predict(X_test_standard)

# 默认的预测指标为分类准确度
knn_clf.score(X_test, y_test)
</code></pre>
<h2 id="12-网格搜索-gridsearchcv">1.2 网格搜索 GridSearchCV</h2>
<p>使用网格搜索来确定KNN算法合适的超参数</p>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'weights':['uniform'],
        'n_neighbors':[ i for i in range(1, 11)]
    },
    {
        'weights':['distance'],
        'n_neighbors':[i for i in range(1, 11)],
        'p':[p for p in range(1, 6)]
    }
]

grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1, verbose=2)
grid_search.fit(X_train_standard, y_train)
knn_clf = grid_search.best_estimator_
knn_clf.score(X_test_standard, y_test)
</code></pre>
<h2 id="13-交叉验证">1.3 交叉验证</h2>
<ul>
<li>
<p>GridSearchCV 本身就包括了交叉验证，也可自己指定参数cv</p>
<p>默认GridSearchCV的KFold平分为3份</p>
</li>
<li>
<p>自己指定交叉验证，查看交叉验证成绩</p>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score

# 默认为分成3份
cross_val_score(knn_clf, X_train, y_train, cv=5)
</code></pre>
<p>这里默认的scoring标准为 <strong>accuracy</strong></p>
<p>有许多可选的参数，具体查看<a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">官方文档</a></p>
</li>
<li>
<p>封装成函数，在fit完模型之后，一次性查看多个评价指标的成绩</p>
<p>这里选的只是针对分类算法的指标，也可以是针对回归，聚类算法的评价指标</p>
</li>
</ul>
<pre><code class="language-python">def cv_score_train_test(model):
    num_cv = 5
    score_list = [&quot;accuracy&quot;,&quot;f1&quot;, &quot;neg_log_loss&quot;, &quot;roc_auc&quot;]
    for score in score_list:
        print(score,&quot;\t train:&quot;,cross_val_score(model, X_train, y_train, cv=num_cv, scoring=score).mean())
        print(score,&quot;\t test:&quot;,cross_val_score(model, X_test, y_test, cv=num_cv, scoring=score).mean())
</code></pre>
<hr>
<h1 id="2-线性回归">2. 线性回归</h1>
<h2 id="21-简单线性回归">2.1 简单线性回归</h2>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression

linreg = LinearRegression()

linreg.fit(X_train, y_train)
</code></pre>
<p>查看截距和系数</p>
<pre><code class="language-python">print linreg.intercept_
print linreg.coef_
lin_reg.score(X_test, y_test)

y_predict = linreg.predict(X_test)
</code></pre>
<h2 id="22-多元线性回归">2.2 多元线性回归</h2>
<p>在更高维度的空间中的“直线”，即数据不只有一个维度，而具有多个维度</p>
<p>代码和上面的简单线性回归相同</p>
<hr>
<h1 id="3-梯度下降法">3. 梯度下降法</h1>
<p>使用梯度下降法之前，需要对数据进行<strong>归一化处理</strong></p>
<h2 id="31-随机梯度下降线性回归">3.1 随机梯度下降线性回归</h2>
<p><strong>SGD_reg</strong></p>
<pre><code class="language-python">from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(max_iter=100)
sgd_reg.fit(X_train_standard, y_train_boston)
sgd_reg.score(X_test_standard, y_test_boston)
</code></pre>
<h2 id="32-确定梯度下降计算的准确性">3.2 确定梯度下降计算的准确性</h2>
<p>以多元线性回归的<strong>目标函数（损失函数）</strong>为例</p>
<p>比较 使用<strong>数学推导式</strong>（得出具体解析解）的方法和<strong>debug的近似方法</strong>的比较</p>
<pre><code class="language-python"># 编写损失函数
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')
        
# 编写梯度函数（使用数学推导方式得到的）
def dJ_math(theta, X_b, y):
    return X_b.T.dot(X_b.dot(theta) - y) * 2.0 / len(y)
    
# 编写梯度函数（用来debug的形式）
def dJ_debug(theta, X_b, y, epsilon=0.01):
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2 * epsilon)
    return res

# 批量梯度下降，寻找最优的theta
def gradient_descent(dJ, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    
    while i_iter &lt; n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        
        if(abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
            break
        
        i_iter += 1
    return theta

# 函数入口参数第一个，要指定dJ函数是什么样的
</code></pre>
<pre><code class="language-python">X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01

# 使用debug方式
theta = gradient_descent(dJ_debug, X_b, y, initial_theta, eta)
# 使用数学推导方式
theta = gradient_descent(dJ_math, X_b, y, initial_theta, eta)
# 得出的这两个theta应该是相同的
</code></pre>
<hr>
<h1 id="4-pca算法">4. PCA算法</h1>
<p>由于是求方差最大，因此使用的是<strong>梯度上升法</strong></p>
<p>PCA算法<strong>不能</strong>在前处理进行<strong>归一化处理</strong>，否则将会找不到主成分</p>
<h2 id="41-代码流程">4.1 代码流程</h2>
<pre><code class="language-python"># 对于二维的数据样本来说
from sklearn.decomposition import PCA

pca = PCA(n_components=1) #指定需要保留的前n个主成分，不指定为默认保留所有
pca.fit(X)
</code></pre>
<p>比如，要使用KNN分类算法，先进行数据的降维操作</p>
<pre><code class="language-python">from sklearn.decomposition import PCA

pca = PCA(n_components=2)  #这里也可以给一个百分比，代表想要保留的数据的方差占比
pca.fit(X_train)

#训练集和测试集需要进行相同降维处理操作
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)

#降维完成后就可以送给模型进行拟合
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction, y_train)
knn_clf.score(X_test_reduction, y_test)
</code></pre>
<h2 id="42-降维的维数和精度的取舍">4.2 降维的维数和精度的取舍</h2>
<p>指定的维数，能解释原数据的方差的<strong>比例</strong></p>
<pre><code class="language-python">pca.explained_variance_ratio_

# 指定保留所有的主成分
pca = PCA(n_components=X_train.shape[1])
pca.fit(X_train)
pca.explained_variance_ratio_

# 查看降维后特征的维数
pca.n_components_
</code></pre>
<p>把数据降维到2维，可以进行scatter的可视化操作</p>
<h2 id="43-pca数据降噪">4.3 PCA数据降噪</h2>
<p>先使用pca降维，之后再反向，升维</p>
<pre><code class="language-python">from sklearn.decomposition import PCA

pca = PCA(0.7)
pca.fit(X)
pca.n_components_

X_reduction = pca.transform(X)
X_inversed = pca.inverse_transform(X_reduction)
</code></pre>
<hr>
<h1 id="5-多项式回归与模型泛化">5. 多项式回归与模型泛化</h1>
<p>多项式回顾需要指定最高的阶数， <strong>degree</strong></p>
<p>拟合的将不再是一条直线</p>
<ul>
<li>只有一个特征的样本，进行多项式回归可以拟合出曲线，并且在二维平面图上进行绘制</li>
<li>而对于具有多个特征的样本，同样可以进行多项式回归，但是不能可视化拟合出来的曲线</li>
</ul>
<h2 id="51-多项式回归和pipeline">5.1 多项式回归和Pipeline</h2>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline


poly_reg = Pipeline([
    (&quot;poly&quot;, PolynomialFeatures(degree=2)),
    (&quot;std_scaler&quot;, StandardScaler()),
    (&quot;lin_reg&quot;, LinearRegression())
])

poly_reg.fit(X, y)
y_predict = poly_reg.predict(X)

# 对二维数据点可以绘制拟合后的图像
plt.scatter(X, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
</code></pre>
<pre><code class="language-python">#更常用的是，把pipeline写在函数中
def PolynomialRegression(degree):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;lin_reg&quot;, LinearRegression())
    ])

poly2_reg = PolynomialRegression(degree=2)
poly2_reg.fit(X, y)

y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)
</code></pre>
<h2 id="52-gridsearchcv-和-pipeline">5.2 GridSearchCV 和 Pipeline</h2>
<p>明确：</p>
<ul>
<li>GridSearchCV：用于寻找给定模型的最优的参数</li>
<li>Pipeline：用于将几个流程整合在一起（PolynomialFeatures()、StandardScaler()、LinearRegression()）</li>
</ul>
<p>如果非要把上两者写在一起，应该把指定好param_grid参数的grid_search作为成员，传递给Pipeline</p>
<h2 id="53-模型泛化之岭回归（ridge）">5.3 模型泛化之岭回归（Ridge）</h2>
<p>首先明确：</p>
<ul>
<li>模型泛化是为了解决<strong>模型过拟合</strong>的问题</li>
<li>岭回归是<strong>模型正则化</strong>的一种处理方式，也称为<strong>L2正则化</strong></li>
<li>岭回归是<strong>线性回归</strong>的一种正则化处理后的模型（作为pipeline的成员使用）</li>
</ul>
<pre><code class="language-python">from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def RidgeRegression(degree, alpha):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;ridge_reg&quot;, Ridge(alpha=alpha))
    ])

ridge_reg = RidgeRegression(degree=20, alpha=0.0001)
ridge_reg.fit(X_train, y_train)

y_predict = ridge_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
</code></pre>
<hr>
<p>代码中：</p>
<p>alpha为L2正则项前面的系数，代表的含义与LASSO回归相同</p>
<ul>
<li>alpha越小，越倾向于选择<strong>复杂模型</strong></li>
<li>alpha越大，越倾向于选择<strong>简单模型</strong></li>
</ul>
<p>Ridge回归、LASSO回归的区别</p>
<ul>
<li>Ridge：更倾向于保持为<strong>曲线</strong></li>
<li>LASSO： 更倾向于变为<strong>直线</strong>（即趋向于使得部分theta变成0， 因此有<strong>特征选择</strong>的作用）</li>
</ul>
<hr>
<h2 id="54-模型泛化之lasso回归">5.4 模型泛化之LASSO回归</h2>
<ul>
<li>岭回归是<strong>模型正则化</strong>的一种处理方式，也称为<strong>L1正则化</strong></li>
<li>岭回归是<strong>线性回归</strong>的一种正则化处理后的模型（作为pipeline的成员使用）</li>
</ul>
<pre><code class="language-python">from sklearn.linear_model import Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def LassoRegression(degree, alpha):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;lasso_reg&quot;, Lasso(alpha=alpha))
    ])

lasso_reg = LassoRegression(3, 0.01)
lasso_reg.fit(X_train, y_train)

y_predict = lasso_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
</code></pre>
<hr>
<h1 id="6-逻辑回归">6. 逻辑回归</h1>
<p>将样本特征与样本发生的<strong>概率</strong>联系起来。</p>
<ul>
<li>既可看做回归算法，也可分类算法</li>
<li>通常作为二分类算法</li>
</ul>
<h2 id="61-绘制决策边界">6.1 绘制决策边界</h2>
<pre><code class="language-python"># 不规则决策边界绘制方法
def plot_decision_boundary(model, axis):

    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])

    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
    
    
#此处为线性逻辑回归
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
</code></pre>
<p>绘制决策边界</p>
<pre><code class="language-python">plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0, 0], X[y==0, 1], color='r')
plt.scatter(X[y==1, 0], X[y==1, 1], color='blue')
plt.show()
</code></pre>
<h2 id="62-多项式逻辑回归">6.2 多项式逻辑回归</h2>
<p>同样，类似于多项式回归，需要使用<strong>Pipeline构造多项式特征项</strong></p>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly',PolynomialFeatures(degree=degree)),
        ('std_scaler',StandardScaler()),
        ('log_reg',LogisticRegression())
    ])

poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X, y)
poly_log_reg.score(X, y)
</code></pre>
<p>如果有需要，可以绘制出决策边界</p>
<pre><code class="language-python">plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()
</code></pre>
<h2 id="63-逻辑回归中的正则化项和惩罚系数c">6.3 逻辑回归中的正则化项和惩罚系数C</h2>
<p>公式为：</p>
<p><strong>C * J(θ) + L1</strong></p>
<p><strong>C * J(θ) + L2</strong></p>
<p>上式中：</p>
<ul>
<li>C越大，L1、L2的作用越弱，模型越倾向<strong>复杂</strong></li>
<li>C越小，相对L1、L2作用越强， J(θ) 作用越弱，模型越倾向<strong>简单</strong></li>
</ul>
<pre><code class="language-python">def PolynomialLogisticRegression(degree, C, penalty='l2'):
    return Pipeline([
        ('poly',PolynomialFeatures(degree=degree)),
        ('std_scaler',StandardScaler()),
        ('log_reg',LogisticRegression(C = C, penalty=penalty))
        # 逻辑回归模型，默认为 penalty='l2'
    ])
</code></pre>
<h2 id="64-ovr-和-ovo">6.4 OVR 和 OVO</h2>
<p>将只适用于二分类的算法，改造为适用于多分类问题</p>
<p>scikit封装了<strong>OvO OvR</strong>这两个类，方便其他二分类算法，使用这两个类实现多分类</p>
<p>例子中：log_reg是已经创建好的逻辑回归二分类器</p>
<pre><code class="language-python">from sklearn.multiclass import OneVsRestClassifier

ovr = OneVsRestClassifier(log_reg)
ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)


from sklearn.multiclass import OneVsOneClassifier

ovo = OneVsOneClassifier(log_reg)
ovo.fit(X_train, y_train)
ovo.score(X_test, y_test)
</code></pre>
<hr>
<h1 id="7-支撑向量机svm">7. 支撑向量机SVM</h1>
<p><strong>注意</strong></p>
<ul>
<li>由于涉及到<strong>距离</strong>的概念，因此，在SVM拟合之前，必须先进行<strong>数据标准化</strong></li>
</ul>
<p>支撑向量机要满足的优化目标是：</p>
<p>使 “<strong>最优决策边界</strong>”  到与两个类别的最近的样本  的距离最远</p>
<p>即，使得 <strong>margin</strong> 最大化</p>
<p>分为：</p>
<ul>
<li>Hard Margin SVM</li>
<li>Soft Margin SVM</li>
</ul>
<h2 id="71-svm的正则化">7.1 SVM的正则化</h2>
<p>为了改善SVM模型的泛化能力，需要进行正则化处理，同样有L1、L2正则化</p>
<p>正则化即弱化限定条件，使得某些样本可以不再Margin区域内</p>
<p>惩罚系数 <strong>C</strong> 是乘在正则项前面的</p>
<p><div class="math display">\[min\frac{1}{2}||w||^2+C\sum_{i=1}^{m}{\xi_i}\text{,L1正则项}
\]</div></p><p><div class="math display">\[min\frac{1}{2}||w||^2+C\sum_{i=1}^{m}{\xi_i^2}  \text {,L2正则项}
\]</div></p><p><strong>变化规律</strong> ：</p>
<ul>
<li>C越大，容错空间越小，越偏向于Hard Margin</li>
<li>C越小，容错空间越大，越偏向于Soft Margin</li>
</ul>
<h2 id="72-线性svm">7.2 线性SVM</h2>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X)
X_standard = standardScaler.transform(X)

from sklearn.svm import LinearSVC
svc = LinearSVC(C=1e9)
svc.fit(X_standard, y)
</code></pre>
<p>简洁起见，可以用Pipeline包装起来</p>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def Linear_svc(C=1.0):
    return Pipeline([
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;linearSVC&quot;, LinearSVC(C=C))
    ])
linear_svc = Linear_svc(C=1e5)
linear_svc.fit(X, y)
</code></pre>
<h2 id="73-多项式特征svm">7.3 多项式特征SVM</h2>
<p><strong>明确：使用多项式核函数的目的都是将数据升维，使得原本线性不可分的数据变得线性可分</strong></p>
<p>在SVM中使用多项式特征有两种方式</p>
<ul>
<li>使用线性SVM，通过pipeline将 **poly  、std 、 linear_svc ** 三个连接起来</li>
<li>使用<strong>多项式核函数SVM</strong>, 则Pipeline只用包装 <strong>std 、 kernelSVC</strong> 两个类</li>
</ul>
<h3 id="731-传统pipeline多项式svm">7.3.1 传统Pipeline多项式SVM</h3>
<pre><code class="language-python"># 传统上使用多项式特征的SVM
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def PolynomialSVC(degree, C=1.0):
    return Pipeline([
        (&quot;ploy&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_standard&quot;, StandardScaler()),
        (&quot;linearSVC&quot;, LinearSVC(C=C))
    ])

poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)
</code></pre>
<h3 id="732-多项式核函数svm">7.3.2 多项式核函数SVM</h3>
<pre><code class="language-python"># 使用多项式核函数的SVM

from sklearn.svm import SVC

def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
        (&quot;std_standard&quot;, StandardScaler()),
        (&quot;kernelSVC&quot;, SVC(kernel='poly', degree=degree, C=C))
    ])

poly_kernel_svc = PolynomialKernelSVC(degree=3)
poly_kernel_svc.fit(X, y)
</code></pre>
<h3 id="733-高斯核svm（rbf）">7.3.3 高斯核SVM（RBF）</h3>
<p>将原本是<span class="math inline">\(m*n\)</span>的数据变为<span class="math inline">\(m*m\)</span></p>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

def RBFkernelSVC(gamma=1.0):
    return Pipeline([
        (&quot;std_standard&quot;, StandardScaler()),
        (&quot;svc&quot;, SVC(kernel=&quot;rbf&quot;, gamma=gamma))
    ])

svc = RBFkernelSVC(gamma=1.0)
svc.fit(X, y)
</code></pre>
<p>超参数gamma  <span class="math inline">\(\gamma\)</span>  规律：</p>
<ul>
<li>gamma越大，高斯核越“窄”，头部越“尖”</li>
<li>gamma越小，高斯核越“宽”，头部越“平缓”，图形叉得越开</li>
</ul>
<p>若gamma太大，会造成 <strong>过拟合</strong></p>
<p>若gamma太小，会造成 <strong>欠拟合</strong> ，决策边界变为 <strong>直线</strong></p>
<h2 id="74-使用svm解决回归问题">7.4 使用SVM解决回归问题</h2>
<p>指定margin区域垂直方向上的距离 <span class="math inline">\(\epsilon\)</span>    epsilon</p>
<p>通用可以分为<strong>线性SVR</strong>和<strong>多项式SVR</strong></p>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVR
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline

def StandardLinearSVR(epsilon=0.1):
    return Pipeline([
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;linearSVR&quot;, LinearSVR(epsilon=epsilon))
    ])

svr = StandardLinearSVR()
svr.fit(X_train, y_train)

svr.score(X_test, y_test)
# 可以使用cross_val_score来获得交叉验证的成绩，成绩更加准确
</code></pre>
<hr>
<h1 id="8-决策树">8. 决策树</h1>
<p>非参数学习算法、天然可解决多分类问题、可解决回归问题(取叶子结点的平均值)、非常容易产生过拟合</p>
<p>可以考虑使用网格搜索来寻找最优的超参数</p>
<p>划分的依据有 基于<strong>信息熵</strong> 、 基于<strong>基尼系数</strong> (scikit默认用gini，两者没有特别优劣之分)</p>
<p>ID3、C4.5都是使用“entropy&quot;评判方式</p>
<p>CART(Classification and Regression Tree)使用的是“gini&quot;评判方式</p>
<p>常用超参数：</p>
<ul>
<li>max_depth</li>
<li>min_samples_split （设置最小的可供继续划分的样本数量 ）</li>
<li>min_samples_leaf （指定叶子结点最小的包含样本的数量 ）</li>
<li>max_leaf_nodes （指定，最多能生长出来的叶子结点的数量 ）</li>
</ul>
<h2 id="81-分类">8.1 分类</h2>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion=&quot;gini&quot;)
# dt_clf = DecisionTreeClassifier(max_depth=2, criterion=&quot;entropy&quot;)

dt_clf.fit(X, y)
</code></pre>
<h2 id="82-回归">8.2 回归</h2>
<pre><code class="language-python">from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)

dt_reg.score(X_test, y_test)
# 计算的是R2值
</code></pre>
<hr>
<h1 id="9-集成学习和随机森林">9. 集成学习和随机森林</h1>
<h2 id="91-hard-voting-classifier">9.1 Hard Voting Classifier</h2>
<p>把几种分类模型包装在一起，根据每种模型的投票结果来得出最终预测类别</p>
<p><strong>可以先使用网格搜索把每种模型的参数调至最优，再来Voting</strong></p>
<pre><code class="language-python">from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

voting_clf = VotingClassifier(estimators=[
    (&quot;log_clf&quot;,LogisticRegression()),
    (&quot;svm_clf&quot;, SVC()),
    (&quot;dt_clf&quot;, DecisionTreeClassifier())
], voting='hard')
voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)
</code></pre>
<h2 id="92-soft-voting-classifier">9.2 Soft Voting Classifier</h2>
<p>更合理的投票应该考虑每种模型的权重，即考虑每种模型对自己分类结果的 <strong>有把握程度</strong></p>
<p>所以，每种模型都应该能估计<strong>结果的概率</strong></p>
<ul>
<li>逻辑回归</li>
<li>KNN</li>
<li>决策树（叶子结点一般不止含有一类数据，因此可以有概率）</li>
<li>SVM中的SVC（可指定probability参数为True）</li>
</ul>
<pre><code class="language-python">soft_voting_clf = VotingClassifier(estimators=[
    (&quot;log_clf&quot;,LogisticRegression()),
    (&quot;svm_clf&quot;, SVC(probability=True)),
    (&quot;dt_clf&quot;, DecisionTreeClassifier(random_state=666))
], voting='soft')

soft_voting_clf.fit(X_train, y_train)
soft_voting_clf.score(X_test, y_test)
</code></pre>
<h2 id="93-bagging（放回取样）">9.3 Bagging（放回取样）</h2>
<p>（1）Bagging(放回取样) 和 Pasting(不放回取样)，由参数 <strong>bootstrap</strong> 来指定</p>
<ul>
<li>True：放回取样</li>
<li>False：不放回取样</li>
</ul>
<p>（2）这类集成学习方法需要指定一个 <strong>base estimator</strong></p>
<p>（3）放回取样，会存在 <strong>oob  (out of bag)</strong> 的样本数据，比例约37%，正好作为测试集</p>
<blockquote>
<p>obb_score=True/False , 是否使用oob作为测试集</p>
</blockquote>
<p>（4）产生差异化的方式：</p>
<ul>
<li>只针对特征进行随机采样：random subspace</li>
<li>既针对样本，又针对特征随机采样： random patches</li>
</ul>
<pre><code class="language-python">random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=500,
                               bootstrap=True, oob_score=True,
                               n_jobs=-1,
                               max_features=1, bootstrap_features=True)
random_subspaces_clf.fit(X, y)
random_subspaces_clf.oob_score_
</code></pre>
<pre><code class="language-python">random_patches_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500, max_samples=100,
                               bootstrap=True, oob_score=True,
                               n_jobs=-1,
                               max_features=1, bootstrap_features=True)
random_patches_clf.fit(X, y)
random_patches_clf.oob_score_
</code></pre>
<p>参数解释：</p>
<blockquote>
<p>max_samples: 如果和样本总数一致，则不进行样本随机采样</p>
<p>max_features: 指定随机采样特征的个数（应小于样本维数）</p>
<p>bootstrap_features: 指定是否进行随机特征采样</p>
<p>oob_score: 指定是都用oob样本来评分</p>
<p>bootstrap: 指定是否进行放回取样</p>
</blockquote>
<h2 id="94-随机森林和extra-tree">9.4 随机森林和Extra-Tree</h2>
<h3 id="941-随机森林">9.4.1 随机森林</h3>
<p>随机森林是指定了 Base Estimator为<strong>Decision Tree</strong> 的Bagging集成学习模型</p>
<p>已经被scikit封装好，可以直接使用</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=500, random_state=666, oob_score=True, n_jobs=-1)
rf_clf.fit(X, y)
rf_clf.oob_score_

#因为随机森林是基于决策树的，因此，决策树的相关参数这里都可以指定修改
rf_clf2 = RandomForestClassifier(n_estimators=500, random_state=666, max_leaf_nodes=16, oob_score=True, n_jobs=-1)
rf_clf2.fit(X, y)
rf_clf.oob_score_
</code></pre>
<h3 id="942-extra-tree">9.4.2 Extra-Tree</h3>
<p>Base Estimator为<strong>Decision Tree</strong> 的Bagging集成学习模型</p>
<p>特点：</p>
<blockquote>
<p>决策树在结点划分上，使用随机的特征和阈值</p>
<p>提供了额外的随机性，可以抑制过拟合，但会增大Bias (偏差)</p>
<p>具有更快的训练速度</p>
</blockquote>
<pre><code class="language-python">from sklearn.ensemble import ExtraTreesRegressor
et_clf = ExtraTreesClassifier(n_estimators=500, bootstrap=True, \
                              oob_score=True, random_state=666)
et_clf.fit(X, y)
et_clf.oob_score_
</code></pre>
<h2 id="95-ada-boosting">9.5 Ada Boosting</h2>
<p>每个子模型模型都在尝试增强（boost）整体的效果，通过不断的模型迭代，更新样本点的<strong>权重</strong></p>
<p>Ada Boosting没有oob的样本，因此需要进行 <strong>train_test_split</strong></p>
<p>需要指定 <strong>Base Estimator</strong></p>
<pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=500)
ada_clf.fit(X_train, y_train)

ada_clf.score(X_test, y_test)
</code></pre>
<h2 id="96-gradient-boosting">9.6 Gradient Boosting</h2>
<p>训练一个模型m1， 产生错误e1</p>
<p>针对e1训练第二个模型m2， 产生错误e2</p>
<p>针对e2训练第二个模型m3， 产生错误e3</p>
<p>......</p>
<p>最终的预测模型是：<span class="math inline">\(m1+m2+m3+...\)</span></p>
<p>Gradient Boosting是基于决策树的，不用指定Base Estimator</p>
<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=30)
gb_clf.fit(X_train, y_train)
gb_clf.score(X_test, y_test)
</code></pre>
<h2 id="总结">总结</h2>
<p>上述提到的集成学习模型，不仅可以用于解决分类问题，也可解决回归问题</p>
<pre><code class="language-python">from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
</code></pre>
<h3 id="例子：">例子：</h3>
<p>决策树和Ada Boosting回归问题效果对比</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor

# 构造测试函数
rng = np.random.RandomState(1)
X = np.linspace(-5, 5, 200)[:, np.newaxis]
y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])

# 回归决策树
dt_reg = DecisionTreeRegressor(max_depth=4)
# 集成模型下的回归决策树
ada_dt_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),
                               n_estimators=200, random_state=rng)

dt_reg.fit(X, y)
ada_dt_reg.fit(X, y)

# 预测
y_1 = dt_reg.predict(X)
y_2 = ada_dt_reg.predict(X)

# 画图
plt.figure()
plt.scatter(X, y, c=&quot;k&quot;, label=&quot;trainning samples&quot;)
plt.plot(X, y_1, c=&quot;g&quot;, label=&quot;n_estimators=1&quot;, linewidth=2)
plt.plot(X, y_2, c=&quot;r&quot;, label=&quot;n_estimators=200&quot;, linewidth=2)
plt.xlabel(&quot;data&quot;)
plt.ylabel(&quot;target&quot;)
plt.title(&quot;Boosted Decision Tree Regression&quot;)
plt.legend()
plt.show()
</code></pre>
<p><img src="https://upload-images.jianshu.io/upload_images/19168686-c59db31b4f642981.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="10-k-means聚类">10. K-means聚类</h1>
<p><a href="https://juejin.im/post/5daffd26e51d45249f6085a5#heading-0">K-means算法实现</a>：文章介绍了k-means算法的基本原理和scikit中封装的kmeans库的基本参数的含义</p>
<p><a href="http://midday.me/article/f8d29baa83ae41ec8c9826401eb7685e">K-means源码解读</a> ： 这篇文章解读了scikit中kmeans的源码</p>
<p>本例的notebook笔记文件：<a href="%5Bhttps://github.com/tangg9646/file_share/blob/master/%E3%80%8Apython%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E6%89%8B%E5%86%8C%E3%80%8BP402.ipynb%5D(https://github.com/tangg9646/file_share/blob/master/%E3%80%8Apython%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E6%89%8B%E5%86%8C%E3%80%8BP402.ipynb)">git仓库</a></p>
<p>实例代码：</p>
<pre><code class="language-python">from matplotlib import pyplot  as plt
from sklearn.metrics import accuracy_score
import numpy as np
import seaborn as sns; sns.set()
%matplotlib inline
</code></pre>
<h2 id="101-传统k-means聚类">10.1 传统K-means聚类</h2>
<p>构造数据集</p>
<pre><code class="language-python">from sklearn.datasets.samples_generator import make_blobs
</code></pre>
<pre><code class="language-python">X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
plt.scatter(X[:,0], X[:, 1], s=50)
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191211110658.png" alt=""></p>
<pre><code class="language-python">from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)
</code></pre>
<p>绘制聚类结果，  画出聚类中心</p>
<pre><code class="language-python">plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

centers = kmeans.cluster_centers_
plt.scatter(centers[:,0], centers[:, 1], c='black', s=80, marker='x')
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191211110732.png" alt=""></p>
<h2 id="102-非线性边界聚类">10.2 非线性边界聚类</h2>
<p>对于非线性边界的kmeans聚类的介绍，查阅于《python数据科学手册》P410</p>
<p>构造数据</p>
<pre><code class="language-python">from sklearn.datasets import make_moons
X, y = make_moons(200, noise=0.05, random_state=0)
</code></pre>
<p>传统kmeans聚类失败的情况</p>
<pre><code class="language-python">labels = KMeans(n_clusters=2, random_state=0).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191211111028.png" alt=""></p>
<p>应用<strong>核方法</strong>， 将数据投影到更高纬的空间，变成线性可分</p>
<pre><code class="language-python">from sklearn.cluster import SpectralClustering
</code></pre>
<pre><code class="language-python">model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')
labels = model.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191211111107.png" alt=""></p>
<h2 id="103-预测结果与真实标签的匹配">10.3 预测结果与真实标签的匹配</h2>
<p>手写数字识别例子</p>
<pre><code class="language-python">from sklearn.datasets import load_digits
digits = load_digits()
</code></pre>
<p>进行聚类</p>
<pre><code class="language-python">kmeans = KMeans(n_clusters=10, random_state=0)
clusters = kmeans.fit_predict(digits.data)
kmeans.cluster_centers_.shape
</code></pre>
<pre><code>(10, 64)
</code></pre>
<p>可以将这些族中心点看做是具有<strong>代表性的数字</strong></p>
<pre><code class="language-python">fig, ax = plt.subplots(2, 5, figsize=(8, 3))
centers = kmeans.cluster_centers_.reshape(10, 8, 8)
for axi, center in zip(ax.flat, centers):
    axi.set(xticks=[], yticks=[])
    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191211111412.png" alt=""></p>
<p><strong>进行众数匹配</strong></p>
<pre><code class="language-python">from scipy.stats import mode

labels = np.zeros_like(clusters)
for i in range(10):
    #得到聚类结果第i类的 True Flase 类型的index矩阵
    mask = (clusters ==i)
    #根据index矩阵，找出这些target中的众数，作为真实的label
    labels[mask] = mode(digits.target[mask])[0]
</code></pre>
<p>有了真实的指标，可以进行准确度计算</p>
<pre><code class="language-python">accuracy_score(digits.target, labels)
</code></pre>
<pre><code>0.7935447968836951
</code></pre>
<h2 id="104-聚类结果的混淆矩阵">10.4 聚类结果的混淆矩阵</h2>
<pre><code class="language-python">from sklearn.metrics import confusion_matrix
mat = confusion_matrix(digits.target, labels)
np.fill_diagonal(mat, 0)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
            xticklabels=digits.target_names,
            yticklabels=digits.target_names)
plt.xlabel('true label')
plt.ylabel('predicted label')
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191211111621.png" alt=""></p>
<h2 id="105-t分布邻域嵌入预处理">10.5 t分布邻域嵌入预处理</h2>
<p>即将高纬的 非线性的数据</p>
<p>通过流形学习</p>
<p>投影到低维空间</p>
<pre><code class="language-python">from sklearn.manifold import TSNE

# 投影数据
# 此过程比较耗时
tsen = TSNE(n_components=2, init='pca', random_state=0)
digits_proj = tsen.fit_transform(digits.data)

#计算聚类的结果
kmeans = KMeans(n_clusters=10, random_state=0)
clusters = kmeans.fit_predict(digits_proj)

#将聚类结果和真实标签进行匹配
labels = np.zeros_like(clusters)
for i in range(10):
    mask = (clusters == i)
    labels[mask] = mode(digits.target[mask])[0]
    
# 计算准确度
accuracy_score(digits.target, labels)
</code></pre>
<h1 id="11-高斯混合模型（聚类、密度估计）">11. 高斯混合模型（聚类、密度估计）</h1>
<p>k-means算法的非概率性和仅根据到族中心的距离指派族的特征导致该算法性能低下</p>
<p>且k-means算法只对简单的，分离性能好的，并且是圆形分布的数据有比较好的效果</p>
<p>本例中所有代码的实现已上传至 <a href="%5Bhttps://github.com/tangg9646/file_share/blob/master/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%EF%BC%88GMM%EF%BC%89.ipynb%5D(https://github.com/tangg9646/file_share/blob/master/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%EF%BC%88GMM%EF%BC%89.ipynb)">git仓库</a></p>
<h2 id="111-观察k-means算法的缺陷">11.1 观察K-means算法的缺陷</h2>
<p>通过实例来观察K-means算法的缺陷</p>
<pre><code class="language-python">%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
</code></pre>
<pre><code class="language-python"># 生成数据点
from sklearn.datasets.samples_generator import make_blobs
X, y_true = make_blobs(n_samples=400, centers=4,
                       cluster_std=0.60, random_state=0)
X = X[:, ::-1] # flip axes for better plotting
</code></pre>
<pre><code class="language-python"># 绘制出kmeans聚类后的标签的结果
from sklearn.cluster import KMeans
kmeans = KMeans(4, random_state=0)
labels = kmeans.fit(X).predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');

centers = kmeans.cluster_centers_
plt.scatter(centers[:,0], centers[:, 1], c='black', s=80, marker='x')
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212161444.png" alt=""></p>
<p>k-means算法相当于在每个族的中心放置了一个圆圈，（针对此处的二维数据来说）</p>
<p>半径是根据最远的点与族中心点的距离算出</p>
<p>下面用一个函数将这个聚类圆圈可视化</p>
<pre><code class="language-python">from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist

def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):
    labels = kmeans.fit_predict(X)

    # plot the input data
    ax = ax or plt.gca()
    ax.axis('equal')
    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)


    # plot the representation of the KMeans model

    centers = kmeans.cluster_centers_
    ax.scatter(centers[:,0], centers[:, 1], c='black', s=150, marker='x')
    
    radii = [cdist(X[labels == i], [center]).max() for i, center in enumerate(centers)]
    #用列表推导式求出每一个聚类中心 i = 0, 1, 2, 3在自己的所属族的距离的最大值
    #labels == i 返回一个布尔型index，所以X[labels == i]只取出i这个族类的数据点
    #求出这些数据点到聚类中心的距离cdist(X[labels == i], [center])  再求最大值 .max()
    
    
    for c, r in zip(centers, radii):
        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))
</code></pre>
<p>如果数据点<strong>不是圆形分布</strong>的</p>
<p>k-means算法的聚类效果就会变差</p>
<pre><code class="language-python">rng = np.random.RandomState(13)
# 这里乘以一个2,2的矩阵，相当于在空间上执行旋转拉伸操作
X_stretched = np.dot(X, rng.randn(2, 2))

kmeans = KMeans(n_clusters=4, random_state=0)
plot_kmeans(kmeans, X_stretched)
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212161553.png" alt=""></p>
<h2 id="112-引出高斯混合模型">11.2 引出高斯混合模型</h2>
<p>高斯混合模型能够计算出每个数据点，属于每个族中心的概率大小</p>
<p>在默认参数设置的、数据简单可分的情况下，</p>
<p>GMM的分类效果与k-means基本相同</p>
<pre><code class="language-python">from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=4).fit(X)
labels = gmm.predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');


#gmm的中心点叫做 means_
centers = gmm.means_
plt.scatter(centers[:,0], centers[:, 1], c='black', s=80, marker='x');
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212161813.png" alt=""></p>
<p>得到数据的<strong>概率分布结果</strong></p>
<pre><code class="language-python">probs = gmm.predict_proba(X)
print(probs[:5].round(3))
</code></pre>
<pre><code>[[0.    0.469 0.    0.531]
 [1.    0.    0.    0.   ]
 [1.    0.    0.    0.   ]
 [0.    0.    0.    1.   ]
 [1.    0.    0.    0.   ]]
</code></pre>
<p>编写绘制gmm绘制边界的函数</p>
<pre><code class="language-python">from matplotlib.patches import Ellipse

def draw_ellipse(position, covariance, ax=None, **kwargs):
    &quot;&quot;&quot;Draw an ellipse with a given position and covariance&quot;&quot;&quot;
    ax = ax or plt.gca()
    
    # Convert covariance to principal axes
    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle = 0
        width, height = 2 * np.sqrt(covariance)
    
    # Draw the Ellipse
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height,
                             angle, **kwargs))
        
def plot_gmm(gmm, X, label=True, ax=None):
    ax = ax or plt.gca()
    labels = gmm.fit(X).predict(X)
    if label:
        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)
    else:
        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
    ax.axis('equal')
    
    w_factor = 0.2 / gmm.weights_.max()
    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):
        draw_ellipse(pos, covar, alpha=w * w_factor)
</code></pre>
<ul>
<li>在圆形数据上的聚类结果</li>
</ul>
<pre><code class="language-python">gmm = GaussianMixture(n_components=4, random_state=42)
plot_gmm(gmm, X)
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212161915.png" alt=""></p>
<ul>
<li>在偏斜拉伸数据上的聚类结果</li>
</ul>
<pre><code class="language-python">gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)
plot_gmm(gmm, X_stretched)
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212161927.png" alt=""></p>
<h2 id="113-将gmm用作密度估计">11.3 将GMM用作密度估计</h2>
<p>GMM本质上是一个<strong>密度估计算法</strong>；也就是说，从技术的角度考虑，</p>
<p>一个 GMM 拟合的结果并不是一个聚类模型，而是描述数据分布的生成概率模型。</p>
<ul>
<li>非线性边界的情况</li>
</ul>
<pre><code class="language-python"># 构建非线性可分数据

from sklearn.datasets import make_moons
Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)
plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212162003.png" alt=""></p>
<p>​	如果使用2个成分聚类（即废了结果设置为2），基本没什么效果</p>
<pre><code class="language-python">gmm2 = GaussianMixture(n_components=2, covariance_type='full', random_state=0)
plot_gmm(gmm2, Xmoon)
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212162020.png" alt=""></p>
<p>​	如果设置为多个聚类成分</p>
<pre><code class="language-python">gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0)
plot_gmm(gmm16, Xmoon, label=False)
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212162051.png" alt=""></p>
<p>这里采用 16 个高斯曲线的混合形式<strong>不是为了找到数据的分隔的簇</strong>，而是为了<strong>对输入数据的总体分布建模</strong>。</p>
<h2 id="114-由分布函数得到生成模型">11.4 由分布函数得到生成模型</h2>
<p>分布函数的生成模型可以生成新的，与输入数据类似的随机分布函数（<strong>生成新的数据点</strong>）</p>
<p><strong>用 GMM 拟合原始数据获得的 16 个成分生成的 400 个新数据点</strong></p>
<pre><code class="language-python">Xnew = gmm16.sample(400)
</code></pre>
<pre><code class="language-python">Xnew[0][:5]
</code></pre>
<pre><code class="language-python">Xnew = gmm16.sample(400)
plt.scatter(Xnew[0][:, 0], Xnew[0][:, 1]);
</code></pre>
<p><img src="https://raw.githubusercontent.com/tangg9646/my_github_image_bed/master/img20191212162308.png" alt=""></p>
<h2 id="115-需要多少成分？">11.5 需要多少成分？</h2>
<p>作为一种生成模型，GMM 提供了一种确定数据集最优成分数量的方法。</p>
<ul>
<li>
<p>赤池信息量准则（Akaike information criterion)  <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a></p>
</li>
<li>
<p>贝叶斯信息准则（Bayesian information criterion) <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a></p>
</li>
</ul>
<pre><code class="language-python">n_components = np.arange(1, 21)
models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(Xmoon)
          for n in n_components]

plt.plot(n_components, [m.bic(Xmoon) for m in models], label='BIC')
plt.plot(n_components, [m.aic(Xmoon) for m in models], label='AIC')
plt.legend(loc='best')
plt.xlabel('n_components');
</code></pre>
<p><img alt="png"></p>
<p>观察可得，在 8~12 个主成分的时候，AIC 较小</p>
<hr>
<h1 id="评价指标">评价指标</h1>
<h2 id="一、分类算法">一、分类算法</h2>
<p><strong>常用指标选择方式</strong></p>
<p>平衡分类问题：</p>
<blockquote>
<p>分类准确度、ROC曲线</p>
</blockquote>
<p>类别不平衡问题：</p>
<blockquote>
<p>精准率、召回率</p>
</blockquote>
<p>对于二分类问题，常用的指标是 f1 、 roc_auc</p>
<p>多分类问题，可用的指标为 <code>f1_weighted</code></p>
<h3 id="1分类准确度">1.分类准确度</h3>
<p>一般用于<strong>平衡分类问题（每个类比的可能性相同）</strong></p>
<pre><code class="language-python">from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_predict)  #(真值，预测值)
</code></pre>
<h3 id="2-混淆矩阵、精准率、召回率">2. 混淆矩阵、精准率、召回率</h3>
<ul>
<li>
<p>精准率：<em><strong>正确预测为1</strong></em>  的数量，占，<em><strong>所有预测为1</strong></em>的比例</p>
<p><img src="https://upload-images.jianshu.io/upload_images/19168686-29bafc200881540f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
<li>
<p>召回率：<em><strong>正确预测为1</strong></em>  的数量，占，  <em><strong>所有确实为1</strong></em>的比例</p>
<p><img src="https://upload-images.jianshu.io/upload_images/19168686-d753338ba9608855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/19168686-06cd0cfd9fa075f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="混淆矩阵.png"></p>
<pre><code class="language-python"># 先真实值，后预测值
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_log_predict)

from sklearn.metrics import precision_score
precision_score(y_test, y_log_predict)

from sklearn.metrics import recall_score
recall_score(y_test, y_log_predict)
</code></pre>
<hr>
<p>多分类问题中的混淆矩阵</p>
<hr>
<ul>
<li>多分类结果的<strong>精准率</strong></li>
</ul>
<pre><code class="language-python">from sklearn.metrics import precision_score
precision_score(y_test, y_predict, average=&quot;micro&quot;)
</code></pre>
<ul>
<li>多分类问题中的<strong>混淆矩阵</strong></li>
</ul>
<pre><code class="language-python">from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
</code></pre>
<ul>
<li>
<p>移除对角线上分类正确的结果，可视化查看其它分类错误的情况</p>
<p>同样，横坐标为<strong>预测值</strong>，纵坐标为<strong>真实值</strong></p>
</li>
</ul>
<pre><code class="language-python">cfm = confusion_matrix(y_test, y_predict)
row_sums = np.sum(cfm, axis=1)
err_matrix = cfm / row_sums
np.fill_diagonal(err_matrix, 0)

plt.matshow(err_matrix, cmap=plt.cm.gray)
plt.show()
</code></pre>
<h3 id="3f1-score">3.F1-score</h3>
<p>F1-score是<strong>精准率precision</strong>和<strong>召回率recall</strong>的调和平均数</p>
<p><img src="https://upload-images.jianshu.io/upload_images/19168686-11bc9fa1d0576a48.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="F1-score"></p>
<pre><code class="language-python">from sklearn.metrics import f1_score

f1_score(y_test, y_predict)
</code></pre>
<h3 id="4精准率和召回率的平衡">4.精准率和召回率的平衡</h3>
<p>可以通过调整阈值，改变精确率和召回率（默认阈值为0）</p>
<ul>
<li>拉高阈值，会提高精准率，降低召回率</li>
<li>降低阈值，会降低精准率，提高召回率</li>
</ul>
<pre><code class="language-python"># 返回模型算法预测得到的成绩
# 这里是以  逻辑回归算法  为例
decision_score = log_reg.decision_function(X_test)

# 调整阈值为5
y_predict_2 = np.array(decision_score &gt;= 5, dtype='int')
# 返回的结果是0 、1
</code></pre>
<h3 id="5精准率-召回率曲线（pr曲线）">5.精准率-召回率曲线（PR曲线）</h3>
<pre><code class="language-python">from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, decision_score)
# 这里的decision_score是上面由模型对X_test预测得到的对象
</code></pre>
<ul>
<li>绘制PR曲线</li>
</ul>
<pre><code class="language-python"># 精确率召回率曲线
plt.plot(precisions, recalls)
plt.show()
</code></pre>
<ul>
<li>将精准率和召回率曲线，绘制在同一张图中</li>
</ul>
<blockquote>
<p>注意，当取“最大的” threshold值的时候，精准率=1，召回率=0，</p>
<p>但是，这个最大的threshold没有对应的值</p>
<p>因此thresholds会少一个</p>
</blockquote>
<pre><code class="language-python">plt.plot(thresholds, precisions[:-1], color='r')
plt.plot(thresholds, recalls[:-1], color='b')
plt.show()
</code></pre>
<h3 id="6roc曲线">6.ROC曲线</h3>
<p>Reciver Operation Characteristic Curve</p>
<ul>
<li>
<p>TPR： True Positive rate</p>
<p><img src="https://upload-images.jianshu.io/upload_images/19168686-44219afbd79abcb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
<li>
<p>FPR： False Positive Rate</p>
<p><div class="math display">\[FPR=\frac{FP}{TN+FP}
\]</div></p><p><img src="https://upload-images.jianshu.io/upload_images/19168686-bdc5e2c5cab4c241.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/19168686-06cd0cfd9fa075f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="混淆矩阵.png"></p>
<p>绘制ROC曲线</p>
<pre><code class="language-python">from sklearn.metrics import roc_curve

fprs, tprs, thresholds = roc_curve(y_test, decision_scores)

plt.plot(fprs, tprs)
plt.show()
</code></pre>
<p>计算ROC曲线下方的面积的函数</p>
<p>roc_ <strong>a</strong>rea_ <strong>u</strong>nder_ <strong>c</strong>urve_score</p>
<pre><code class="language-python">from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, decision_scores)
</code></pre>
<p>曲线下方的面积可用于比较两个模型的好坏</p>
<p>总之，上面提到的decision_score 是一个概率值，如0 1 二分类问题，应该是将每个样本预测为1的概率，</p>
<p>如某个样本的y_test为1，y_predict_probablity为0.875</p>
<p>每个测试样本对应一个预测的概率值</p>
<p>通常在模型fit完成之后，都会有相应的得到概率的函数，如</p>
<p><strong>model.predict_prob(X_test)</strong></p>
<p><strong>model.decision_function(X_test)</strong></p>
<h2 id="二、回归算法">二、回归算法</h2>
<h3 id="1均方误差-mse">1.均方误差 MSE</h3>
<pre><code class="language-python">from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_predict)
</code></pre>
<h3 id="2平均绝对值误差-mae">2.平均绝对值误差 MAE</h3>
<pre><code class="language-python">from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test, y_predict)
</code></pre>
<h3 id="3均方根误差-rmse">3.均方根误差 RMSE</h3>
<p>scikit中没有单独定于均方根误差，需要自己对均方误差MSE开平方根</p>
<h3 id="4r2评分">4.R2评分</h3>
<pre><code class="language-python">from sklearn.metrics import r2_score
r2_score(y_test, y_predict)
</code></pre>
<h3 id="5学习曲线">5.学习曲线</h3>
<p>观察模型在<strong>训练数据集</strong>和<strong>测试数据集</strong>上的评分，随着<strong>训练数据集样本数增加</strong>的变化趋势。</p>
<pre><code class="language-python">import numpy as np
import matplot.pyplot as plt
from sklearn.metrics import mean_squared_error


def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
    
    train_score = []
    test_score = []
    for i in range(1, len(X_train)+1):
        algo.fit(X_train[:i], y_train[:i])

        y_train_predict = algo.predict(X_train[:i])
        train_score.append(mean_squared_error(y_train[:i], y_train_predict))

        y_test_predict = algo.predict(X_test)
        test_score.append(mean_squared_error(y_test, y_test_predict))
    
    plt.plot([i for i in range(1, len(X_train)+1)], np.sqrt(train_score), label=&quot;train&quot;)
    plt.plot([i for i in range(1, len(X_train)+1)], np.sqrt(test_score), label=&quot;test&quot;)
    plt.legend()
    plt.axis([0,len(X_train)+1, 0, 4])

    plt.show()
 
# 调用
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test )
</code></pre>

</div>
<div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
    <div id="blog_post_info"></div>
    <div class="clear"></div>
    <div id="post_next_prev"></div>
</div>
            </div>
            <div class="postDesc">posted @ 
<span id="post-date">2020-04-26 22:31</span>&nbsp;
<a href="https://www.cnblogs.com/tangg/">唐啊唐囧囧</a>&nbsp;
阅读(<span id="post_view_count">...</span>)&nbsp;
评论(<span id="post_comment_count">...</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=12782862" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(12782862);return false;">收藏</a></div>
        </div>
<script src="https://common.cnblogs.com/highlight/9.12.0/highlight.min.js"></script>
<script>markdown_highlight();</script>
<script>
    var allowComments = true, cb_blogId = 566870, cb_blogApp = 'tangg', cb_blogUserGuid = '07c9bac8-3fef-4600-c2a9-08d77885435f';
    var cb_entryId = 12782862, cb_entryCreatedDate = '2020-04-26 22:31', cb_postType = 1; 
    loadViewCount(cb_entryId);
</script><a name="!comments"></a>
<div id="blog-comments-placeholder"></div>
<script>
    var commentManager = new blogCommentManager();
    commentManager.renderComments(0);
</script>

<div id="comment_form" class="commentform">
    <a name="commentform"></a>
    <div id="divCommentShow"></div>
    <div id="comment_nav"><span id="span_refresh_tips"></span><a href="javascript:void(0);" onclick="return RefreshCommentList();" id="lnk_RefreshComments" runat="server" clientidmode="Static">刷新评论</a><a href="#" onclick="return RefreshPage();">刷新页面</a><a href="#top">返回顶部</a></div>
    <div id="comment_form_container"></div>
    <div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
    <div id="ad_t2"></div>
    <div id="opt_under_post"></div>
    <script async="async" src="https://www.googletagservices.com/tag/js/gpt.js"></script>
    <script>
        var googletag = googletag || {};
        googletag.cmd = googletag.cmd || [];
    </script>
    <script>
        googletag.cmd.push(function () {
            googletag.defineSlot("/1090369/C1", [300, 250], "div-gpt-ad-1546353474406-0").addService(googletag.pubads());
            googletag.defineSlot("/1090369/C2", [468, 60], "div-gpt-ad-1539008685004-0").addService(googletag.pubads());
            googletag.pubads().enableSingleRequest();
            googletag.enableServices();
        });
    </script>
    <div id="cnblogs_c1" class="c_ad_block">
        <div id="div-gpt-ad-1546353474406-0" style="height:250px; width:300px;"></div>
    </div>
    <div id="under_post_news"></div>
    <div id="cnblogs_c2" class="c_ad_block">
        <div id="div-gpt-ad-1539008685004-0" style="height:60px; width:468px;">
            <script>
                if (new Date() >= new Date(2018, 9, 13)) {
                    googletag.cmd.push(function () { googletag.display("div-gpt-ad-1539008685004-0"); });
                }
            </script>
        </div>
    </div>
    <div id="under_post_kb"></div>
    <div id="HistoryToday" class="c_ad_block"></div>
    <script type="text/javascript">
        fixPostBody();
        deliverBigBanner();
setTimeout(function() { incrementViewCount(cb_entryId); }, 50);        deliverAdT2();
        deliverAdC1();
        deliverAdC2();
        loadNewsAndKb();
        loadBlogSignature();
LoadPostCategoriesTags(cb_blogId, cb_entryId);        LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
        GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
        loadOptUnderPost();
        GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);
    </script>
</div>    </div>
</div>
            </div>
        </div>

        <div id="sideBar">
            <div id="sideBarMain">
                
<div id="sidebar_news" class="newsItem">
            <script>loadBlogNews();</script>
</div>

                <div id="calendar"><div id="blog-calendar" style="display:none"></div></div>                
                <script>loadBlogDefaultCalendar();</script>
                <div id="leftcontentcontainer">
                    <!-- begin:SingleColumn -->
                    <div id="blog-sidecolumn"></div>
                    <script>loadBlogSideColumn();</script>
                    <!-- end:  SingleColumn -->
                </div>
            </div>
        </div>
        <div class="clear"></div>
    </div>
    <div class="clear"></div>
    <div id="footer">
        <!--done-->
Copyright &copy; 2020 唐啊唐囧囧
<br /><span id="poweredby">Powered by .NET Core on Kubernetes</span>

    </div>
</div>

    
</body>
</html>